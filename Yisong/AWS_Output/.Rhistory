pred.band = predict(model, newdata, interval = "prediction")
plot(y=cats$Hwt, x=cats$Bwt, main="Scatterplot of Cats Dataset",
xlab="Body Weight (kg)", ylab="Heart Weight (g)")
abline(model, lty = 2)
lines(newdata$Bwt, conf.band[, 2], col = "blue") #Plotting the lower confidence band.
lines(newdata$Bwt, conf.band[, 3], col = "blue") #Plotting the upper confidence band.
lines(newdata$Bwt, pred.band[, 2], col = "red") #Plotting the lower prediction band.
lines(newdata$Bwt, pred.band[, 3], col = "red") #Plotting the upper prediction band.
legend("topleft", c("Regression Line", "Conf. Band", "Pred. Band"),
lty = c(2, 1, 1), col = c("black", "blue", "red"))
View(conf.band)
newdata
conf.band
model
summary(model)
newdata = data.frame("cats$Bwt" = seq(1, 5, 0.3))
newdata
newdata = data.frame(Bwt = seq(1, 5, 0.3))
newdata
help(predict)
predict(model, newdata)
warnings()
predict(model, cats$Bwt = 5)
predict(model, "cats$Bwt" = 5)
dim(cats)
predict(model)
model <- lm(formula = cats$Hwt ~ cats$Bwt, data = cats)
newdata = data.frame(Bwt = seq(1, 5, 0.3))
conf.band = predict(model, newdata, interval = "confidence")
pred.band = predict(model, newdata, interval = "prediction")
plot(y=cats$Hwt, x=cats$Bwt, main="Scatterplot of Cats Dataset",
xlab="Body Weight (kg)", ylab="Heart Weight (g)")
abline(model, lty = 2)
lines(newdata$Bwt, conf.band[, 2], col = "blue") #Plotting the lower confidence band.
lines(newdata$Bwt, conf.band[, 3], col = "blue") #Plotting the upper confidence band.
lines(newdata$Bwt, pred.band[, 2], col = "red") #Plotting the lower prediction band.
lines(newdata$Bwt, pred.band[, 3], col = "red") #Plotting the upper prediction band.
legend("topleft", c("Regression Line", "Conf. Band", "Pred. Band"),
lty = c(2, 1, 1), col = c("black", "blue", "red"))
rm (list = ls())
#Basic Summary
names(cats)
summary(cats)
ggplot(cats, aes(y=cats$Hwt, x=cats$Bwt))+
geom_point() +
coord_cartesian() +
scale_color_gradient() +
theme_bw()
#Linear Regression would be a good fit for the model
#relationship appears to be linear, data appears to be correlated
#2
model <- lm(formula = cats$Hwt ~ cats$Bwt, data = cats)
#2a, #2b
```
Coefficients:
(Intercept)     cats$Bwt
-0.3567       4.0341
Heart Weight=0, Body Weight=-.3567
Heart Weight=1kg, Body Weight = 4.0341
```
summary(models)
#2c
```
the slope is significant, it has a higher p-value
the intercept is not significant, it has a small p-value
the slope is a reliable measure, the intercept is not
```
#2d
```
the f statistic is very small confirming that a linear relationship
can explain some of the relationiship between the variables
```
#2e
```
RSE is 1.452 meaning on average points fall this distance
away from the line of regression
```
#2f
```
R**2 is .6466 meaning the linear model explains 64.66% of the
relationship between heart weight and body weight
```
#3
ggplot(cats,aes(Bwt, Hwt)) +
geom_point() +
geom_smooth(method = "lm")
#4
ggplot(model,aes(model$fitted.values,model$residuals)) +
geom_point()
#none seem abnormally large
#5
confint(model)
```
2.5 %   97.5 %
(Intercept) -1.725163 1.011838
cats$Bwt     3.539343 4.528782
We are 95% sure the intercept lies between -1.7 and 1.01
We are 95% sure the slope lies between 3.54 and 4.53
```
#6
#Assumption of linearity based on plot initial plot
qqnorm(model$residuals)
#Assumption of constant variance and independent errors
qqline(model$residuals)
#Assumption of normality is a little questionable on the right side
#7
newdata = data.frame(Bwt = seq(1, 5, 0.3))
conf.band = predict(model, newdata, interval = "confidence")
pred.band = predict(model, newdata, interval = "prediction")
plot(y=cats$Hwt, x=cats$Bwt, main="Scatterplot of Cats Dataset",
xlab="Body Weight (kg)", ylab="Heart Weight (g)")
abline(model, lty = 2)
lines(newdata$Bwt, conf.band[, 2], col = "blue") #Plotting the lower confidence band.
lines(newdata$Bwt, conf.band[, 3], col = "blue") #Plotting the upper confidence band.
lines(newdata$Bwt, pred.band[, 2], col = "red") #Plotting the lower prediction band.
lines(newdata$Bwt, pred.band[, 3], col = "red") #Plotting the upper prediction band.
legend("topleft", c("Regression Line", "Conf. Band", "Pred. Band"),
lty = c(2, 1, 1), col = c("black", "blue", "red"))
newdata = data.frame(Bwt = seq(1, 5, 0.3))
conf.band = predict(model, newdata, interval = "confidence")
lty = c(2, 1, 1), col = c("black", "blue", "red"))
newdata = data.frame(Bwt = seq(1, 4, 0.2))
conf.band = predict(model, newdata, interval = "confidence")
pred.band = predict(model, newdata, interval = "prediction")
plot(y=cats$Hwt, x=cats$Bwt, main="Scatterplot of Cats Dataset",
xlab="Body Weight (kg)", ylab="Heart Weight (g)")
abline(coef(model), lty = 2)
lines(newdata$Bwt, conf.band[, 2], col = "blue") #Plotting the lower confidence band.
lines(newdata$Bwt, conf.band[, 3], col = "blue") #Plotting the upper confidence band.
lines(newdata$Bwt, pred.band[, 2], col = "red") #Plotting the lower prediction band.
lines(newdata$Bwt, pred.band[, 3], col = "red") #Plotting the upper prediction band.
legend("topleft", c("Regression Line", "Conf. Band", "Pred. Band"),
lty = c(2, 1, 1), col = c("black", "blue", "red"))
model <- lm(formula = cats$Hwt ~ cats$Bwt, data = cats)
data(cats)
model <- lm(Hwt ~ Bwt, data = cats)
newdata = data.frame(Bwt = seq(1, 5, 0.3))
conf.band = predict(model, newdata, interval = "confidence")
pred.band = predict(model, newdata, interval = "prediction")
plot(y=cats$Hwt, x=cats$Bwt, main="Scatterplot of Cats Dataset",
xlab="Body Weight (kg)", ylab="Heart Weight (g)")
abline(model, lty = 2)
lines(newdata$Bwt, conf.band[, 2], col = "blue") #Plotting the lower confidence band.
lines(newdata$Bwt, conf.band[, 3], col = "blue") #Plotting the upper confidence band.
lines(newdata$Bwt, pred.band[, 2], col = "red") #Plotting the lower prediction band.
lines(newdata$Bwt, pred.band[, 3], col = "red") #Plotting the upper prediction band.
legend("topleft", c("Regression Line", "Conf. Band", "Pred. Band"),
lty = c(2, 1, 1), col = c("black", "blue", "red"))
newdata2 = data.frame(Bwt = c(2.8, 5, 10)) #Creating a new data frame to pass
predict(model, newdata2, interval = "confidence")
predict(model, newdata2, interval = "prediction")
plot(y=cats$Hwt, x=cats$Bwt, main="Scatterplot of Cats Dataset",
xlab="Body Weight (kg)", ylab="Heart Weight (g)")
abline(model, lty = 2)
lines(newdata2$Bwt, conf.band[, 2], col = "blue") #Plotting the lower confidence band.
lines(newdata2$Bwt, conf.band[, 3], col = "blue") #Plotting the upper confidence band.
lines(newdata2$Bwt, pred.band[, 2], col = "red") #Plotting the lower prediction band.
lines(newdata2$Bwt, pred.band[, 3], col = "red") #Plotting the upper prediction band.
legend("topleft", c("Regression Line", "Conf. Band", "Pred. Band"),
lty = c(2, 1, 1), col = c("black", "blue", "red"))
newdata = data.frame(Bwt = seq(1, 5, 0.3))
conf.band = predict(model, newdata, interval = "confidence")
pred.band = predict(model, newdata, interval = "prediction")
plot(y=cats$Hwt, x=cats$Bwt, main="Scatterplot of Cats Dataset",
xlab="Body Weight (kg)", ylab="Heart Weight (g)")
abline(model, lty = 2)
lines(newdata$Bwt, conf.band[, 2], col = "blue") #Plotting the lower confidence band.
lines(newdata$Bwt, conf.band[, 3], col = "blue") #Plotting the upper confidence band.
lines(newdata$Bwt, pred.band[, 2], col = "red") #Plotting the lower prediction band.
lines(newdata$Bwt, pred.band[, 3], col = "red") #Plotting the upper prediction band.
legend("topleft", c("Regression Line", "Conf. Band", "Pred. Band"),
lty = c(2, 1, 1), col = c("black", "blue", "red"))
library(ggplot2)
library(dplyr)
library(MASS)
data(cats)
##########################################
####          Question 1             #####
##########################################
#Basic Summary
names(cats)
summary(cats)
ggplot(cats, aes(y=cats$Hwt, x=cats$Bwt))+
geom_point() +
coord_cartesian() +
scale_color_gradient() +
theme_bw()
#Linear Regression would be a good fit for the model
#relationship appears to be linear, data appears to be correlated
#3
ggplot(cats,aes(Bwt, Hwt)) +
geom_point() +
geom_smooth(method = "lm")
#4
ggplot(model,aes(model$fitted.values,model$residuals)) +
geom_point()
#none seem abnormally large
model <- lm(Hwt ~ Bwt, data = cats)
summary(model)
qqline(model$residuals)
#Assumption of normality is a little questionable on the right side
library(ggplot2)
library(dplyr)
library(MASS)
data(cats)
##########################################
####          Question 1             #####
##########################################
#Basic Summary
names(cats)
summary(cats)
ggplot(cats, aes(y=cats$Hwt, x=cats$Bwt))+
geom_point() +
coord_cartesian() +
scale_color_gradient() +
theme_bw()
#Linear Regression would be a good fit for the model
#relationship appears to be linear, data appears to be correlated
#2
model <- lm(Hwt ~ Bwt, data = cats)
#2a, #2b
```
Coefficients:
(Intercept)     cats$Bwt
-0.3567       4.0341
Heart Weight=0, Body Weight=-.3567
Heart Weight=1kg, Body Weight = 4.0341
```
summary(model)
#2c
```
the slope is significant, it has a higher p-value
the intercept is not significant, it has a small p-value
the slope is a reliable measure, the intercept is not
```
#2d
```
the f statistic is very small confirming that a linear relationship
can explain some of the relationiship between the variables
```
#2e
```
RSE is 1.452 meaning on average points fall this distance
away from the line of regression
```
#2f
```
R**2 is .6466 meaning the linear model explains 64.66% of the
relationship between heart weight and body weight
```
#3
ggplot(cats,aes(Bwt, Hwt)) +
geom_point() +
geom_smooth(method = "lm")
#4
ggplot(model,aes(model$fitted.values,model$residuals)) +
geom_point()
#none seem abnormally large
#5
confint(model)
```
2.5 %   97.5 %
(Intercept) -1.725163 1.011838
cats$Bwt     3.539343 4.528782
We are 95% sure the intercept lies between -1.7 and 1.01
We are 95% sure the slope lies between 3.54 and 4.53
```
#6
#Assumption of linearity based on plot initial plot
qqnorm(model$residuals)
#Assumption of constant variance and independent errors
qqline(model$residuals)
#Assumption of normality is a little questionable on the right side
#7
newdata = data.frame(Bwt = seq(1, 5, 0.3))
conf.band = predict(model, newdata, interval = "confidence")
pred.band = predict(model, newdata, interval = "prediction")
plot(y=cats$Hwt, x=cats$Bwt, main="Scatterplot of Cats Dataset",
xlab="Body Weight (kg)", ylab="Heart Weight (g)")
abline(model, lty = 2)
lines(newdata$Bwt, conf.band[, 2], col = "blue") #Plotting the lower confidence band.
lines(newdata$Bwt, conf.band[, 3], col = "blue") #Plotting the upper confidence band.
lines(newdata$Bwt, pred.band[, 2], col = "red") #Plotting the lower prediction band.
lines(newdata$Bwt, pred.band[, 3], col = "red") #Plotting the upper prediction band.
legend("topleft", c("Regression Line", "Conf. Band", "Pred. Band"),
lty = c(2, 1, 1), col = c("black", "blue", "red"))
#the purpose of the prediction band is to capture the individual values with 95% confidence
#the purpose of the confidence band is to capture the mean value with 95% confidence
#there is less of a variation in mean than in individual values
installed.packages("ggmap")
install.packages('ggmap')
library(ggmap)
manualCoordinates <- geocode("67A Van Reipen Ave, Jersey City, NJ")
manualCoordinates
install.packages('psych')
library(psych) #Library that contains helpful PCA functions, such as:
principal() #Performs principal components analysis with optional rotation.
fa.parallel() #Creates scree plots with parallell analyses for choosing K.
factor.plot() #Visualizes the principal component loadings.
install.packages('glmnet')
install.packages('ISLR')
install.packages('caret')
#######################################################
#######################################################
#####[07] The Curse of Dimensionality Lecture Code#####
#######################################################
#######################################################
#######################
#####Tools for PCA#####
#######################
library(psych) #Library that contains helpful PCA functions, such as:
principal() #Performs principal components analysis with optional rotation.
fa.parallel() #Creates scree plots with parallell analyses for choosing K.
factor.plot() #Visualizes the principal component loadings.
############################
#####Data for Example 1#####
############################
bodies = Harman23.cor$cov #Covariance matrix of 8 physical measurements on 305 girls.
bodies
####################
#####Choosing K#####
####################
fa.parallel(bodies, #The data in question.
n.obs = 305, #Since we supplied a covaraince matrix, need to know n.
fa = "pc", #Display the eigenvalues for PCA.
n.iter = 100) #Number of simulated analyses to perform.
abline(h = 1) #Adding a horizontal line at 1.
#1. Kaiser-Harris criterion suggests retaining PCs with eigenvalues > 1; PCs with
#   eigenvalues < 1 explain less varaince than contained in a single variable.
#2. Cattell Scree test visually inspects the elbow graph for diminishing return;
#   retain PCs before a drastic drop-off.
#3. Run simulations and extract eigenvalues from random data matrices of the same
#   dimension as your data; find where the parallel analysis overshadows real data.
########################
#####Performing PCA#####
########################
pc_bodies = principal(bodies, #The data in question.
nfactors = 2, #The number of PCs to extract.
rotate = "none")
pc_bodies
#-PC columns contain loadings; correlations of the observed variables with the PCs.
#-h2 column displays the component comunalities; amount of variance explained by
# the components.
#-u2 column is the uniqueness (1 - h2); amount of varaince NOT explained by the
# components.
#-SS loadings row shows the eigenvalues of the PCs; the standardized varaince.
#-Proportion/Cumulative Var row shows the variance explained by each PC.
#-Proportion Explained/Cumulative Proportion row considers only the selected PCs.
########################################
#####Visualizing & Interpreting PCA#####
########################################
factor.plot(pc_bodies,
labels = colnames(bodies)) #Add variable names to the plot.
#-PC1 correlates highly positively with length-related variables (height, arm
# span, forearm, and lower leg). This is a "length" dimension.
#-PC2 correlates highly positively with volume-related variables (weight, bitro
# diameter, chest girth, and chest width). This is a "volume" dimension.
############################
#####Data for Example 2#####
############################
iris_meas = iris[, -5] #Measurements of iris dataset.
iris_meas
plot(iris_meas)
####################
#####Choosing K#####
####################
fa.parallel(iris_meas, #The data in question.
fa = "pc", #Display the eigenvalues for PCA.
n.iter = 100) #Number of simulated analyses to perform.
abline(h = 1) #Adding a horizontal line at 1.
#Should extract 1 PC, but let's look at 2.
########################
#####Performing PCA#####
########################
pc_iris = principal(iris_meas, #The data in question.
nfactors = 2,
rotate = "none") #The number of PCs to extract.
pc_iris
factor.plot(pc_iris,
labels = colnames(iris_meas)) #Add variable names to the plot.
#-PC1 separates out the importance of the sepal width as cotrasted with the
# remaining variables.
#-PC2 contrasts the differences between the sepal and petal measurements.
################################
#####Viewing Projected Data#####
################################
plot(iris_meas) #Original data: 4 dimensions.
plot(pc_iris$scores) #Projected data: 2 dimensions.
############################
#####Data for Example 3#####
############################
library(Sleuth2)
case1701
printer_data = case1701[, 1:11]
fa.parallel(printer_data, #The data in question.
fa = "pc", #Display the eigenvalues for PCA.
n.iter = 100) #Number of simulated analyses to perform.
abline(h = 1) #Adding a horizontal line at 1.
#Should extract 1 PC, but let's look at 3.
pc_printer = principal(printer_data, #The data in question.
nfactors = 3,
rotate = "none") #The number of PCs to extract.
pc_printer
factor.plot(pc_printer) #Add variable names to the plot.
#-PC1 ends up being a weighted average.
#-PC2 contrasts one side of the rod with the other.
#-PC3 contrasts the middle of the rod with the sides of the rod.
plot(printer_data)
pairs(pc_printer$scores)
##########################
#####Ridge Regression#####
##########################
library(ISLR)
Hitters = na.omit(Hitters)
help(Hitters)
#Need matrices for glmnet() function. Automatically conducts conversions as well
#for factor variables into dummy variables.
x = model.matrix(Salary ~ ., Hitters)[, -1] #Dropping the intercept column.
y = Hitters$Salary
#Values of lambda over which to check.
grid = 10^seq(5, -2, length = 100)
#Fitting the ridge regression. Alpha = 0 for ridge regression.
library(glmnet)
ridge.models = glmnet(x, y, alpha = 0, lambda = grid)
dim(coef(ridge.models)) #20 different coefficients, estimated 100 times --
#once each per lambda value.
coef(ridge.models) #Inspecting the various coefficient estimates.
#What do the estimates look like for a smaller value of lambda?
ridge.models$lambda[80] #Lambda = 0.2595.
coef(ridge.models)[, 80] #Estimates not close to 0.
sqrt(sum(coef(ridge.models)[-1, 80]^2)) #L2 norm is 136.8179.
#What do the estimates look like for a larger value of lambda?
ridge.models$lambda[15] #Lambda = 10,235.31.
coef(ridge.models)[, 15] #Most estimates close to 0.
sqrt(sum(coef(ridge.models)[-1, 15]^2)) #L2 norm is 7.07.
#Visualizing the ridge regression shrinkage.
plot(ridge.models, xvar = "lambda", label = TRUE, main = "Ridge Regression")
#Can use the predict() function to obtain ridge regression coefficients for a
#new value of lambda, not necessarily one that was within our grid:
predict(ridge.models, s = 50, type = "coefficients")
#Creating training and testing sets. Here we decide to use a 70-30 split with
#approximately 70% of our data in the training set and 30% of our data in the
#test set.
set.seed(0)
train = sample(1:nrow(x), 7*nrow(x)/10)
test = (-train)
y.test = y[test]
length(train)/nrow(x)
length(y.test)/nrow(x)
#Let's attempt to fit a ridge regression using some arbitrary value of lambda;
#we still have not yet figured out what the best value of lambda should be!
#We will arbitrarily choose 5. We will now use the training set exclusively.
ridge.models.train = glmnet(x[train, ], y[train], alpha = 0, lambda = grid)
ridge.lambda5 = predict(ridge.models.train, s = 5, newx = x[test, ])
mean((ridge.lambda5 - y.test)^2)
#Here, the MSE is approximately 115,541.
#What would happen if we fit a ridge regression with an extremely large value
#of lambda? Essentially, fitting a model with only an intercept:
ridge.largelambda = predict(ridge.models.train, s = 1e10, newx = x[test, ])
mean((ridge.largelambda - y.test)^2)
rm(list = ls())
library(tree)
#Loading the ISLR library in order to use the Carseats dataset.
library(ISLR)
install.packages('tree')
library(tree)
help(Carseats)
attach(Carseats)
Salsa
Sales
hist(Sales)
summary(Sales)
High = ifelse(Sales <= 8, "No", "Yes")
Carseats = data.frame(Carseats, High)
tree.carseats = tree(High ~ . - Sales, split = "gini", data = Carseats)
summary(tree.carseats)
plot(tree.carseats)
text(tree.carseats, pretty = 0)
tree.carseats
set.seed(0)
train = sample(1:nrow(Carseats), 7*nrow(Carseats)/10) #Training indices.
Carseats.test = Carseats[-train, ] #Test dataset.
High.test = High[-train] #Test response.
tree.carseats = tree(High ~ . - Sales, data = Carseats, subset = train)
plot(tree.carseats)
text(tree.carseats, pretty = 0)
summary(tree.carseats)
tree.carseats
tree.pred = predict(tree.carseats, Carseats.test, type = "class")
tree.pred
library('HSAUR')
install.packaegs('HSAUR')
install.packages('HSAUR')
data(heptathlon)
?heptathlon
library('HSAUR')
?heptathlon
plot(heptathlon)
install.packages('psych')
?fa.parallel
?fa.parallel
library(psych)
?fa.parallel
install.packages('neuralnet')
install.packages('caret')
install.packages(c("colorspace", "curl", "dygraphs", "evaluate", "htmlwidgets", "knitr", "openssl", "quantmod", "R6", "Rcpp", "reshape2", "rmarkdown", "shiny", "slam", "SparseM", "stringi", "yaml"))
install.packages('xQuartz')
install.packages('XQuartz')
require(devtools)
install_github("Gastrograph/RS3")
setwd("~/github/joyfre/Yisong/AWS_Output")
sfeats <- readRDS('sfeats_random.RDS')
